{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7ba8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nablasquared.com/are-you-tired-of-scrolling-down-through-logs-when-training-an-ml-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83ea6dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ciprian-Florin Ifrim\\AppData\\Local\\Temp\\ipykernel_17796\\2130031179.py:37: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------LIBRARIES---------------------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#!pip install torch torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "#-------------------------------------------------------------------------------------JUPYTER NOTEBOOK SETTINGS-------------------------------------------------------------------------------------\n",
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d12b8ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check cuda availability\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1deec29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x1cf6d097310>, <torch.cuda.device at 0x1cf6d095450>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3f38738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GPU 1/2: NVIDIA GeForce RTX 4090\n",
      "Memory Usage:\n",
      "Allocated: 0.2 GB\n",
      "Cached:    0.4 GB\n",
      "-------------------------------------\n",
      "GPU 2/2: NVIDIA GeForce RTX 4090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        torch.cuda.set_device(device)\n",
    "        print(f'GPU {i+1}/{torch.cuda.device_count()}: {torch.cuda.get_device_name(i)}')\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,1), 'GB')\n",
    "        print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a48f6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_CLASSES = {\"name\": 1, \"value\": 2, \"x-axis\": 3, \"y-axis\": 4, \"plot\":5}\n",
    "PASSTHROUGH_FIELDS = ['folder', 'filename', 'source', 'size', 'segmented', 'object']\n",
    "\n",
    "def transform_voc_target(target, width, height):\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for obj in target[\"annotation\"][\"object\"]:\n",
    "        class_name = obj[0]\n",
    "        bbox = obj[-1]\n",
    "        # Normalize the bounding box coordinates\n",
    "        boxes.append([float(bbox[\"xmin\"]) / width, float(bbox[\"ymin\"]) / height, float(bbox[\"xmax\"]) / width, float(bbox[\"ymax\"]) / height])\n",
    "        if class_name in CUSTOM_CLASSES:\n",
    "            labels.append(CUSTOM_CLASSES[class_name])\n",
    "        else:\n",
    "            print(f\"Warning: {class_name} is not in CUSTOM_CLASSES\")\n",
    "            # you might want to handle this situation better\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    # Hash the filename to a unique numeric value\n",
    "    image_id = torch.tensor([hash(target[\"annotation\"][\"filename\"])])\n",
    "    target = {}\n",
    "    target[\"boxes\"] = boxes\n",
    "    target[\"labels\"] = labels\n",
    "    target[\"image_id\"] = image_id\n",
    "    return target\n",
    "\n",
    "class CustomVOCDetection(Dataset):\n",
    "    def __init__(self, root, dataset_name, image_set='train', transforms=None, classes=None):\n",
    "        self.root = root\n",
    "        self.classes = classes\n",
    "        \n",
    "        voc_root = os.path.join(self.root, 'VOCdevkit', dataset_name)\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets', 'Main')\n",
    "        \n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        if len(img.getbands()) != 3:\n",
    "            print(f\"Image at {self.images[index]} does not have 3 channels after conversion to RGB\")\n",
    "            \n",
    "        # Get the original image size\n",
    "        width, height = img.size\n",
    "        target = self.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            print(f'Image shape after transform: {img.shape}')  # Debugging print\n",
    "        target = transform_voc_target(target, width, height)         # Pass the width and height to the function\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def parse_voc_xml(self, node):\n",
    "        voc_dict = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic = collections.defaultdict(list)\n",
    "            for dc in map(self.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag in PASSTHROUGH_FIELDS:\n",
    "                voc_dict[node.tag] = [def_dic[ind][0] if len(def_dic[ind]) == 1 else def_dic[ind] for ind in def_dic]\n",
    "            else:\n",
    "                voc_dict[node.tag] = {ind: def_dic[ind][0] if len(def_dic[ind]) == 1 else def_dic[ind] for ind in def_dic}\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15b4e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found at pytorch_rcnn_checkpoints/FINAL_rcnn_batch_32_epoch-60_full-enchanced-original_augmented-3X/ or loading failed. Starting from scratch. Error: list index out of range\n",
      "Using 2 GPUs!\n",
      "GPU 1/2: NVIDIA GeForce RTX 4090\n",
      "Memory Usage:\n",
      "Allocated: 0.2 GB\n",
      "Cached:    0.4 GB\n",
      "-------------------------------------\n",
      "GPU 2/2: NVIDIA GeForce RTX 4090\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|                     | 0/25356 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 1/25356 [00:00<2:19:09,  3.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 2/25356 [00:00<2:11:04,  3.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 3/25356 [00:00<2:11:57,  3.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 4/25356 [00:01<2:11:20,  3.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 5/25356 [00:01<2:11:11,  3.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 6/25356 [00:01<2:10:43,  3.23batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 7/25356 [00:02<2:08:21,  3.29batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 8/25356 [00:02<2:06:38,  3.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|           | 9/25356 [00:02<2:08:45,  3.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|          | 10/25356 [00:03<2:11:19,  3.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|          | 11/25356 [00:03<2:13:12,  3.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|          | 12/25356 [00:03<2:11:58,  3.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|          | 13/25356 [00:04<2:11:55,  3.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Error during forward pass: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\", line 83, in forward\n",
      "    images, targets = self.transform(images, targets)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 139, in forward\n",
      "    image = self.normalize(image)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ciprian-Florin Ifrim\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torchvision\\models\\detection\\transform.py\", line 167, in normalize\n",
      "    return (image - mean[:, None, None]) / std[:, None, None]\n",
      "            ~~~~~~^~~~~~~~~~~~~~~~~~~~~\n",
      "RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
      "\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n",
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training epoch 1/60:   0%|          | 13/25356 [00:04<2:12:38,  3.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape after transform: torch.Size([3, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m loss_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    115\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m    118\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m    119\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[49], line 51\u001b[0m, in \u001b[0;36mCustomVOCDetection.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 51\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img\u001b[38;5;241m.\u001b[39mgetbands()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have 3 channels after conversion to RGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch-cuda-env\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Multiprocessing\n",
    "if mp.get_start_method(allow_none=True) != 'spawn':\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "num_classes = 6\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Put model to device\n",
    "model.to(device)\n",
    "\n",
    "# Data transform\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])  # mean values for ImageNet\n",
    "std = torch.tensor([0.229, 0.224, 0.225])   # standard deviation values for ImageNet\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((512, 512)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Apply this function to your dataset using the transforms parameter\n",
    "train_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsEnchanced_Original_With-3X-Augmentation\",\n",
    "    image_set=\"train\",\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "# Add validation data loader\n",
    "val_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsEnchanced_Original_With-3X-Augmentation\",\n",
    "    image_set=\"val\",\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "data_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_data_loader = DataLoader(val_data, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Debugging tensor data\n",
    "# for i, (images, targets) in enumerate(data_loader):\n",
    "#     print(f'Batch index {i}:')\n",
    "#     print('Image:', images)\n",
    "#     print('Targets:', targets)\n",
    "#     if i >= 1:  # print only first 2 batches\n",
    "#         break\n",
    "\n",
    "# Define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "#optimizer = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0005)\n",
    "\n",
    "# Initialize the gradient scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Initialize loss histories for plotting\n",
    "loss_hist = []\n",
    "valid_loss_hist = []\n",
    "\n",
    "# Add a path for the checkpoint\n",
    "MODEL_NAME = \"FINAL_rcnn_batch_32_epoch-60_full-enchanced-original_augmented-3X\"\n",
    "MODEL_EXTENSION = \".pt\"\n",
    "MODEL_SAVE_PATH = \"pytorch_rcnn_models/\" + MODEL_NAME + MODEL_EXTENSION\n",
    "CHECKPOINT_PATH = \"pytorch_rcnn_checkpoints/\" + MODEL_NAME + \"/\"\n",
    "\n",
    "# Load the latest checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    try:\n",
    "        checkpoint_files = [f for f in os.listdir(CHECKPOINT_PATH) if f.endswith('.pth')]\n",
    "        checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))  # sort by epoch number\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        checkpoint = torch.load(os.path.join(CHECKPOINT_PATH, latest_checkpoint))\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        loss_hist = checkpoint['loss_hist']\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No checkpoint found at {CHECKPOINT_PATH} or loading failed. Starting from scratch. Error: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No checkpoint directory found at {CHECKPOINT_PATH}. Starting from scratch.\")\n",
    "    \n",
    "# Wrap the model for usage with multiple GPUs if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 60\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f'GPU {i+1}/{torch.cuda.device_count()}: {torch.cuda.get_device_name(i)}')\n",
    "            print('Memory Usage:')\n",
    "            print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,1), 'GB')\n",
    "            print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,1), 'GB')\n",
    "            print('-------------------------------------')\n",
    "            \n",
    "    # Model Training\n",
    "    torch.cuda.empty_cache()         # clear cuda cache\n",
    "    model.train()\n",
    "\n",
    "    loss_epoch = []\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Training epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            with autocast():\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            continue\n",
    "\n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_value = losses.item()\n",
    "        loss_epoch.append(loss_value)\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss_value})\n",
    "\n",
    "    epoch_loss = sum(loss_epoch)/len(loss_epoch)\n",
    "    loss_hist.append(epoch_loss)\n",
    "    print(f\"Epoch loss: {epoch_loss}\")\n",
    "    \n",
    "    # Validation loop    \n",
    "    torch.cuda.empty_cache()         # clear cuda cache\n",
    "    model.eval()\n",
    "    valid_loss_epoch = []\n",
    "    valid_progress_bar = tqdm(valid_data_loader, desc=f\"Validation epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in valid_progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            valid_loss_value = losses.item()\n",
    "            valid_loss_epoch.append(valid_loss_value)\n",
    "            valid_progress_bar.set_postfix({\"validation_batch_loss\": valid_loss_value})\n",
    "\n",
    "    valid_epoch_loss = sum(valid_loss_epoch)/len(valid_loss_epoch)\n",
    "    valid_loss_hist.append(valid_epoch_loss)\n",
    "    print(f\"Validation Epoch loss: {valid_epoch_loss}\")\n",
    "\n",
    "    # Save the model checkpoint at the end of each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': epoch_loss,\n",
    "        'loss_hist': loss_hist,  \n",
    "    }, os.path.join(CHECKPOINT_PATH, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(\"The model has been saved!\")\n",
    "\n",
    "# Configure Seaborn\n",
    "sns.set_theme()\n",
    "\n",
    "# Create a pandas DataFrame for loss history\n",
    "df_loss = pd.DataFrame(data={'Epoch': range(1, num_epochs + 1), 'Training Loss': loss_hist, 'Validation Loss': valid_loss_hist})\n",
    "\n",
    "# Plotting the loss using seaborn\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=df_loss, x='Epoch', y='Training Loss', color='orange', label='Training Loss')\n",
    "sns.lineplot(data=df_loss, x='Epoch', y='Validation Loss', color='blue', label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('training_loss_plots/' + MODEL_NAME + \".png\", dpi=300)\n",
    "\n",
    "# Show the plot(which also resets the current figure and axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEPENDENT MODEL LOADER FOR TESTING PURPOSES\n",
    "# Load the pretrained model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "# Replace the classifier with a new one\n",
    "CUSTOM_CLASSES = {\"name\": 1, \"value\": 2, \"x-axis\": 3, \"y-axis\": 4, \"plot\":5}\n",
    "num_classes = len(CUSTOM_CLASSES) + 1\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Put model to device \n",
    "model.to(device)\n",
    "\n",
    "# Load saved model\n",
    "model.load_state_dict(torch.load('pytorch_rcnn_models/EXPERIMENTAL_4_rcnn_batch-16_epoch-20_full-crypto.com_non-augmented.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467743ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsNoAugmentation\",\n",
    "    image_set=\"val\",\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "# DataLoader for test data\n",
    "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Get the number of classes from the test dataset - hardcoded as the num of classes are known -> speeds up processing\n",
    "class_correct = [0.] + [0. for _ in range(len(CUSTOM_CLASSES))]\n",
    "class_total = [0.] + [0. for _ in range(len(CUSTOM_CLASSES))]\n",
    "\n",
    "# Testing\n",
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "for images, targets in test_data_loader:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "\n",
    "    # for each output result\n",
    "    for i, output_dict in enumerate(output):\n",
    "        scores = output_dict['scores']\n",
    "        labels = output_dict['labels']\n",
    "\n",
    "        # If the model did not detect any objects, continue to the next image\n",
    "        if scores.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # get the label with the highest score\n",
    "        max_score_index = scores.argmax()\n",
    "        pred_label = labels[max_score_index]\n",
    "\n",
    "        # compare predictions to true label\n",
    "        for true_label in targets[i]['labels']:\n",
    "            correct = (pred_label == true_label).item()\n",
    "\n",
    "            # calculate test accuracy for each object class\n",
    "            class_correct[true_label.item()] += correct\n",
    "            class_total[true_label.item()] += 1\n",
    "\n",
    "# Loop over classes in the dictionary\n",
    "for class_name in CUSTOM_CLASSES.keys():\n",
    "    i = CUSTOM_CLASSES[class_name]  # Get class index from class name\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            class_name, 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (class_name))\n",
    "\n",
    "total_correct = np.sum(class_correct)\n",
    "total = np.sum(class_total)\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * total_correct / total if total > 0 else 0,\n",
    "    total_correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output and targets to the COCO format\n",
    "def to_coco_format(images, output, targets):\n",
    "    coco_format_output = []\n",
    "    coco_format_targets = []\n",
    "    ann_id = 1  # Initialize annotation id\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        image_id = targets[i]['image_id'].item()  # Use image_id from target\n",
    "        image_size = images[i].shape[-2:]\n",
    "        \n",
    "        # Convert output\n",
    "        for box, label, score in zip(output[i][\"boxes\"], output[i][\"labels\"], output[i][\"scores\"]):\n",
    "            box = box.tolist()\n",
    "            label = label.item()\n",
    "            score = score.item()\n",
    "\n",
    "            # Convert to [x, y, width, height]\n",
    "            box[2] -= box[0]\n",
    "            box[3] -= box[1]\n",
    "\n",
    "            # Create COCO-style detection\n",
    "            coco_format_output.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': label,\n",
    "                'bbox': box,\n",
    "                'score': score,\n",
    "                'id': ann_id  # Add 'id' field\n",
    "            })\n",
    "\n",
    "            ann_id += 1  # Increment annotation id\n",
    "\n",
    "        # Convert targets\n",
    "        for box, label in zip(targets[i][\"boxes\"], targets[i][\"labels\"]):\n",
    "            box = box.tolist()\n",
    "            label = label.item()\n",
    "\n",
    "            # Convert to [x, y, width, height]\n",
    "            box[2] -= box[0]\n",
    "            box[3] -= box[1]\n",
    "\n",
    "            # Create COCO-style annotation\n",
    "            coco_format_targets.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': label,\n",
    "                'bbox': box,\n",
    "                'id': ann_id  # Add 'id' field\n",
    "            })\n",
    "\n",
    "            ann_id += 1  # Increment annotation id\n",
    "\n",
    "    return coco_format_output, coco_format_targets\n",
    "\n",
    "# Define test_data_loader\n",
    "test_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsNoAugmentation\",\n",
    "    image_set=\"test\",  # Using 'test' here\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "model.eval()\n",
    "\n",
    "coco_gt = COCO()  # COCO ground truth\n",
    "coco_dt = COCO()  # COCO detections\n",
    "for images, targets in test_data_loader:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "        \n",
    "    # Check if there are any detections\n",
    "    for out in output:\n",
    "        if len(out['boxes']) > 0:\n",
    "            print('Detections made')\n",
    "            break\n",
    "    else:\n",
    "        print('No detections made')\n",
    "\n",
    "    coco_format_output, coco_format_targets = to_coco_format(images, output, targets)\n",
    "\n",
    "    # Define category dictionary\n",
    "    categories = [{'id': i, 'name': name} for i, name in enumerate(CUSTOM_CLASSES)]\n",
    "\n",
    "    # Load results into COCO objects\n",
    "    coco_gt.dataset = {'annotations': coco_format_targets, 'categories': categories}\n",
    "    coco_dt.dataset = {'annotations': coco_format_output, 'categories': categories}\n",
    "\n",
    "    coco_gt.createIndex()\n",
    "    coco_dt.createIndex()\n",
    "\n",
    "    # Create COCO evaluator and evaluate\n",
    "    coco_eval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "# Extract Precision-Recall data from coco_eval object\n",
    "precision = coco_eval.eval['precision']\n",
    "recall = coco_eval.params.recThrs\n",
    "\n",
    "print(len(coco_format_output)) \n",
    "\n",
    "# Number of categories\n",
    "num_categories = precision.shape[2]\n",
    "\n",
    "# Create a figure for the plots\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# For each category\n",
    "for category in range(num_categories):\n",
    "    # Get precision for this category\n",
    "    precision_per_category = precision[:, :, category, 0, -1] # We select the max detection per image (-1)\n",
    "    # Compute average precision across all recall thresholds\n",
    "    avg_precision = precision_per_category.mean(axis=0)\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(recall, avg_precision, label=CUSTOM_CLASSES[category])\n",
    "\n",
    "# Set up labels and title\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Per Category Precision-Recall Curve')\n",
    "ax.legend()  # Add a legend\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to manually clear the CUDA(VRAM) and RAM cache in case of issues or KeyboardInterrupt\n",
    "\n",
    "# delete model or unnecessary tensors\n",
    "#del model\n",
    "\n",
    "# python garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# clear cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# detach tensors\n",
    "#tensor = tensor.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cude-env",
   "language": "python",
   "name": "pytorch-cude-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
