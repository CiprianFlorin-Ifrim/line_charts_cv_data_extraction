{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nablasquared.com/are-you-tired-of-scrolling-down-through-logs-when-training-an-ml-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------LIBRARIES---------------------------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#!pip install torch torchvision\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "#-------------------------------------------------------------------------------------JUPYTER NOTEBOOK SETTINGS-------------------------------------------------------------------------------------\n",
    "from IPython.core.display import display, HTML                                    \n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cuda availability\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43141202",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f38738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        device = torch.device(f'cuda:{i}')\n",
    "        torch.cuda.set_device(device)\n",
    "        print(f'GPU {i+1}/{torch.cuda.device_count()}: {torch.cuda.get_device_name(i)}')\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,1), 'GB')\n",
    "        print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_CLASSES = {\"name\": 1, \"value\": 2, \"x-axis\": 3, \"y-axis\": 4, \"plot\":5}\n",
    "PASSTHROUGH_FIELDS = ['folder', 'filename', 'source', 'size', 'segmented', 'object']\n",
    "\n",
    "def transform_voc_target(target, width, height):\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for obj in target[\"annotation\"][\"object\"]:\n",
    "        class_name = obj[0]\n",
    "        bbox = obj[-1]\n",
    "        # Normalize the bounding box coordinates\n",
    "        boxes.append([float(bbox[\"xmin\"]) / width, float(bbox[\"ymin\"]) / height, float(bbox[\"xmax\"]) / width, float(bbox[\"ymax\"]) / height])\n",
    "        if class_name in CUSTOM_CLASSES:\n",
    "            labels.append(CUSTOM_CLASSES[class_name])\n",
    "        else:\n",
    "            print(f\"Warning: {class_name} is not in CUSTOM_CLASSES\")\n",
    "            # you might want to handle this situation better\n",
    "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    # Hash the filename to a unique numeric value\n",
    "    image_id = torch.tensor([hash(target[\"annotation\"][\"filename\"])])\n",
    "    target = {}\n",
    "    target[\"boxes\"] = boxes\n",
    "    target[\"labels\"] = labels\n",
    "    target[\"image_id\"] = image_id\n",
    "    return target\n",
    "\n",
    "class CustomVOCDetection(Dataset):\n",
    "    def __init__(self, root, dataset_name, image_set='train', transforms=None, classes=None):\n",
    "        self.root = root\n",
    "        self.classes = classes\n",
    "        \n",
    "        voc_root = os.path.join(self.root, 'VOCdevkit', dataset_name)\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets', 'Main')\n",
    "        \n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        if len(img.getbands()) != 3:\n",
    "            print(f\"Image at {self.images[index]} does not have 3 channels after conversion to RGB\")\n",
    "            \n",
    "        # Get the original image size\n",
    "        width, height = img.size\n",
    "        target = self.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            #print(f'Image shape after transform: {img.shape}')  # Debugging print\n",
    "        target = transform_voc_target(target, width, height)         # Pass the width and height to the function\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def parse_voc_xml(self, node):\n",
    "        voc_dict = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic = collections.defaultdict(list)\n",
    "            for dc in map(self.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag in PASSTHROUGH_FIELDS:\n",
    "                voc_dict[node.tag] = [def_dic[ind][0] if len(def_dic[ind]) == 1 else def_dic[ind] for ind in def_dic]\n",
    "            else:\n",
    "                voc_dict[node.tag] = {ind: def_dic[ind][0] if len(def_dic[ind]) == 1 else def_dic[ind] for ind in def_dic}\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def calculate_iou(target_boxes, pred_boxes):\n",
    "    iou_values = []\n",
    "    for target_box in target_boxes:\n",
    "        for pred_box in pred_boxes:\n",
    "            # Compute the intersection\n",
    "            inter_rect_xmin = max(target_box[0], pred_box[0])\n",
    "            inter_rect_ymin = max(target_box[1], pred_box[1])\n",
    "            inter_rect_xmax = min(target_box[2], pred_box[2])\n",
    "            inter_rect_ymax = min(target_box[3], pred_box[3])\n",
    "\n",
    "            inter_area = max(0, inter_rect_xmax - inter_rect_xmin) * max(0, inter_rect_ymax - inter_rect_ymin)\n",
    "\n",
    "            # Compute the union\n",
    "            target_area = (target_box[2] - target_box[0]) * (target_box[3] - target_box[1])\n",
    "            pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "            union_area = target_area + pred_area - inter_area\n",
    "\n",
    "            # Compute the IoU\n",
    "            iou = inter_area / union_area\n",
    "            iou_values.append(iou)\n",
    "    \n",
    "    return torch.tensor(iou_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4e6be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multiprocessing\n",
    "if mp.get_start_method(allow_none=True) != 'spawn':\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    \n",
    "# Check device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the pretrained model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "num_classes = 6\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Put model to device\n",
    "model.to(device)\n",
    "\n",
    "# Data processing\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(512),                                            \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply this function to your dataset using the transforms parameter\n",
    "train_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsEnchanced_Original_NoAugmentation\",\n",
    "    image_set=\"experimental\",\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "val_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsEnchanced_Original_NoAugmentation\",\n",
    "    image_set=\"experimental\",  # assuming the set name is 'validation'\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "# Use collate_fn in DataLoader\n",
    "train_data_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_data_loader = DataLoader(val_data, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Debugging tensor data\n",
    "# for i, (images, targets) in enumerate(data_loader):\n",
    "#     print(f'Batch index {i}:')\n",
    "#     print('Image:', images)\n",
    "#     print('Targets:', targets)\n",
    "#     if i >= 1:  # print only first 2 batches\n",
    "#         break\n",
    "\n",
    "# Define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "#optimizer = torch.optim.Adam(params, lr=0.0001, weight_decay=0.0005)\n",
    "\n",
    "# Initialize the gradient scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Define loss history for plotting\n",
    "loss_hist = []\n",
    "# Initialize validation loss history for plotting\n",
    "valid_loss_hist = []\n",
    "\n",
    "# Add a path for the checkpoint\n",
    "MODEL_NAME = \"EXPERIMENTAL_5_rcnn_batch-16_epoch-20_part-enchanced_non-augmented\"\n",
    "MODEL_EXTENSION = \".pt\"\n",
    "MODEL_SAVE_PATH = \"pytorch_rcnn_models/\" + MODEL_NAME + MODEL_EXTENSION\n",
    "CHECKPOINT_PATH = \"pytorch_rcnn_checkpoints/\" + MODEL_NAME + \"/\"\n",
    "\n",
    "# Load the latest checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    try:\n",
    "        checkpoint_files = [f for f in os.listdir(CHECKPOINT_PATH) if f.endswith('.pth')]\n",
    "        checkpoint_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))  # sort by epoch number\n",
    "        latest_checkpoint = checkpoint_files[-1]\n",
    "        checkpoint = torch.load(os.path.join(CHECKPOINT_PATH, latest_checkpoint))\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        loss_hist = checkpoint['loss_hist']\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No checkpoint found at {CHECKPOINT_PATH} or loading failed. Starting from scratch. Error: {str(e)}\")\n",
    "else:\n",
    "    print(f\"No checkpoint directory found at {CHECKPOINT_PATH}. Starting from scratch.\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "iou_threshold = 0.5  # set IoU threshold\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f'GPU {i+1}/{torch.cuda.device_count()}: {torch.cuda.get_device_name(i)}')\n",
    "            print('Memory Usage:')\n",
    "            print('Allocated:', round(torch.cuda.memory_allocated(i)/1024**3,1), 'GB')\n",
    "            print('Cached:   ', round(torch.cuda.memory_reserved(i)/1024**3,1), 'GB')\n",
    "            print('-------------------------------------')\n",
    "            \n",
    "    # Model Training\n",
    "    torch.cuda.empty_cache()         # clear cuda cache\n",
    "    model.train()\n",
    "    loss_epoch = []\n",
    "    progress_bar = tqdm(train_data_loader, desc=f\"Training epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            with autocast():\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            continue\n",
    "\n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_value = losses.item()\n",
    "        loss_epoch.append(loss_value)\n",
    "        progress_bar.set_postfix({\"batch_loss\": loss_value})\n",
    "\n",
    "    epoch_loss = sum(loss_epoch)/len(loss_epoch)\n",
    "    loss_hist.append(epoch_loss)\n",
    "    print(f\"Epoch loss: {epoch_loss}\")\n",
    "    \n",
    "    # Validation Loop\n",
    "    torch.cuda.empty_cache()         # clear cuda cache\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_data_loader, desc=f\"Validating epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for images, targets in progress_bar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
    "\n",
    "            # Forward\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            valid_loss += losses.item()\n",
    "\n",
    "        # Average validation loss\n",
    "        valid_loss /= len(val_data_loader)\n",
    "        valid_loss_hist.append(valid_loss)\n",
    "        print(f\"Validation loss: {valid_loss}\")\n",
    "    \n",
    "    # Accuracy loop\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         accuracy_epoch = []\n",
    "#         progress_bar = tqdm(val_data_loader, desc=\"Calculating accuracy\", unit=\"batch\")\n",
    "\n",
    "#         for images, targets in progress_bar:\n",
    "#             images = list(image.to(device) for image in images)\n",
    "#             targets = [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
    "\n",
    "#             predictions = model(images)\n",
    "\n",
    "#             for pred, target in zip(predictions, targets):\n",
    "#                 pred_boxes = pred['boxes']\n",
    "#                 target_boxes = target['boxes']\n",
    "#                 ious = calculate_iou(target_boxes, pred_boxes)\n",
    "\n",
    "#                 correct_preds = ious > iou_threshold\n",
    "#                 accuracy = correct_preds.sum().item() / len(pred_boxes)\n",
    "#                 accuracy_epoch.append(accuracy)\n",
    "\n",
    "#             progress_bar.set_postfix({\"batch_accuracy\": accuracy})\n",
    "\n",
    "#         accuracy_per_epoch = sum(accuracy_epoch) / len(accuracy_epoch)\n",
    "#         print(f\"Accuracy per Epoch: {accuracy_per_epoch}\")\n",
    "\n",
    "\n",
    "    # Save the model checkpoint at the end of each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': epoch_loss,\n",
    "        'loss_hist': loss_hist,  \n",
    "        'valid_loss_hist': valid_loss_hist,\n",
    "    }, os.path.join(CHECKPOINT_PATH, f\"checkpoint_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "# Save the model after training\n",
    "torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(\"The model has been saved!\")\n",
    "\n",
    "# Configure Seaborn\n",
    "sns.set_theme()\n",
    "\n",
    "# Create a pandas DataFrame for loss history\n",
    "df_loss = pd.DataFrame(data={'Epoch': range(1, num_epochs + 1), 'Training Loss': loss_hist, 'Validation Loss': valid_loss_hist})\n",
    "\n",
    "# Plotting the loss using seaborn\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=df_loss, x='Epoch', y='Training Loss', color='orange', label='Training Loss')\n",
    "sns.lineplot(data=df_loss, x='Epoch', y='Validation Loss', color='blue', label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('training_loss_plots/' + MODEL_NAME + \".png\", dpi=300)\n",
    "\n",
    "# Show the plot(which also resets the current figure and axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEPENDENT MODEL LOADER FOR TESTING PURPOSES\n",
    "# Load the pretrained model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "# Replace the classifier with a new one\n",
    "CUSTOM_CLASSES = {\"name\": 1, \"value\": 2, \"x-axis\": 3, \"y-axis\": 4, \"plot\":5}\n",
    "num_classes = len(CUSTOM_CLASSES) + 1\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Put model to device \n",
    "model.to(device)\n",
    "\n",
    "# Load saved model\n",
    "model.load_state_dict(torch.load('pytorch_rcnn_models/EXPERIMENTAL_4_rcnn_batch-16_epoch-20_full-crypto.com_non-augmented.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467743ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsNoAugmentation\",\n",
    "    image_set=\"val\",\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "# DataLoader for test data\n",
    "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Get the number of classes from the test dataset - hardcoded as the num of classes are known -> speeds up processing\n",
    "class_correct = [0.] + [0. for _ in range(len(CUSTOM_CLASSES))]\n",
    "class_total = [0.] + [0. for _ in range(len(CUSTOM_CLASSES))]\n",
    "\n",
    "# Testing\n",
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "for images, targets in test_data_loader:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "\n",
    "    # for each output result\n",
    "    for i, output_dict in enumerate(output):\n",
    "        scores = output_dict['scores']\n",
    "        labels = output_dict['labels']\n",
    "\n",
    "        # If the model did not detect any objects, continue to the next image\n",
    "        if scores.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # get the label with the highest score\n",
    "        max_score_index = scores.argmax()\n",
    "        pred_label = labels[max_score_index]\n",
    "\n",
    "        # compare predictions to true label\n",
    "        for true_label in targets[i]['labels']:\n",
    "            correct = (pred_label == true_label).item()\n",
    "\n",
    "            # calculate test accuracy for each object class\n",
    "            class_correct[true_label.item()] += correct\n",
    "            class_total[true_label.item()] += 1\n",
    "\n",
    "# Loop over classes in the dictionary\n",
    "for class_name in CUSTOM_CLASSES.keys():\n",
    "    i = CUSTOM_CLASSES[class_name]  # Get class index from class name\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            class_name, 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (class_name))\n",
    "\n",
    "total_correct = np.sum(class_correct)\n",
    "total = np.sum(class_total)\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * total_correct / total if total > 0 else 0,\n",
    "    total_correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output and targets to the COCO format\n",
    "def to_coco_format(images, output, targets):\n",
    "    coco_format_output = []\n",
    "    coco_format_targets = []\n",
    "    ann_id = 1  # Initialize annotation id\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        image_id = targets[i]['image_id'].item()  # Use image_id from target\n",
    "        image_size = images[i].shape[-2:]\n",
    "        \n",
    "        # Convert output\n",
    "        for box, label, score in zip(output[i][\"boxes\"], output[i][\"labels\"], output[i][\"scores\"]):\n",
    "            box = box.tolist()\n",
    "            label = label.item()\n",
    "            score = score.item()\n",
    "\n",
    "            # Convert to [x, y, width, height]\n",
    "            box[2] -= box[0]\n",
    "            box[3] -= box[1]\n",
    "\n",
    "            # Create COCO-style detection\n",
    "            coco_format_output.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': label,\n",
    "                'bbox': box,\n",
    "                'score': score,\n",
    "                'id': ann_id  # Add 'id' field\n",
    "            })\n",
    "\n",
    "            ann_id += 1  # Increment annotation id\n",
    "\n",
    "        # Convert targets\n",
    "        for box, label in zip(targets[i][\"boxes\"], targets[i][\"labels\"]):\n",
    "            box = box.tolist()\n",
    "            label = label.item()\n",
    "\n",
    "            # Convert to [x, y, width, height]\n",
    "            box[2] -= box[0]\n",
    "            box[3] -= box[1]\n",
    "\n",
    "            # Create COCO-style annotation\n",
    "            coco_format_targets.append({\n",
    "                'image_id': image_id,\n",
    "                'category_id': label,\n",
    "                'bbox': box,\n",
    "                'id': ann_id  # Add 'id' field\n",
    "            })\n",
    "\n",
    "            ann_id += 1  # Increment annotation id\n",
    "\n",
    "    return coco_format_output, coco_format_targets\n",
    "\n",
    "# Define test_data_loader\n",
    "test_data = CustomVOCDetection(\n",
    "    root=\"pascal_voc_datasets/\",\n",
    "    dataset_name=\"PlotsNoAugmentation\",\n",
    "    image_set=\"test\",  # Using 'test' here\n",
    "    transforms=data_transforms,\n",
    "    classes=CUSTOM_CLASSES \n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "model.eval()\n",
    "\n",
    "coco_gt = COCO()  # COCO ground truth\n",
    "coco_dt = COCO()  # COCO detections\n",
    "for images, targets in test_data_loader:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "        \n",
    "    # Check if there are any detections\n",
    "    for out in output:\n",
    "        if len(out['boxes']) > 0:\n",
    "            print('Detections made')\n",
    "            break\n",
    "    else:\n",
    "        print('No detections made')\n",
    "\n",
    "    coco_format_output, coco_format_targets = to_coco_format(images, output, targets)\n",
    "\n",
    "    # Define category dictionary\n",
    "    categories = [{'id': i, 'name': name} for i, name in enumerate(CUSTOM_CLASSES)]\n",
    "\n",
    "    # Load results into COCO objects\n",
    "    coco_gt.dataset = {'annotations': coco_format_targets, 'categories': categories}\n",
    "    coco_dt.dataset = {'annotations': coco_format_output, 'categories': categories}\n",
    "\n",
    "    coco_gt.createIndex()\n",
    "    coco_dt.createIndex()\n",
    "\n",
    "    # Create COCO evaluator and evaluate\n",
    "    coco_eval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "    \n",
    "# Extract Precision-Recall data from coco_eval object\n",
    "precision = coco_eval.eval['precision']\n",
    "recall = coco_eval.params.recThrs\n",
    "\n",
    "print(len(coco_format_output)) \n",
    "\n",
    "# Number of categories\n",
    "num_categories = precision.shape[2]\n",
    "\n",
    "# Create a figure for the plots\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# For each category\n",
    "for category in range(num_categories):\n",
    "    # Get precision for this category\n",
    "    precision_per_category = precision[:, :, category, 0, -1] # We select the max detection per image (-1)\n",
    "    # Compute average precision across all recall thresholds\n",
    "    avg_precision = precision_per_category.mean(axis=0)\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(recall, avg_precision, label=CUSTOM_CLASSES[category])\n",
    "\n",
    "# Set up labels and title\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Per Category Precision-Recall Curve')\n",
    "ax.legend()  # Add a legend\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to manually clear the CUDA(VRAM) and RAM cache in case of issues or KeyboardInterrupt\n",
    "\n",
    "# delete model or unnecessary tensors\n",
    "#del model\n",
    "\n",
    "# python garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# clear cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# detach tensors\n",
    "#tensor = tensor.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cude-env",
   "language": "python",
   "name": "pytorch-cude-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
